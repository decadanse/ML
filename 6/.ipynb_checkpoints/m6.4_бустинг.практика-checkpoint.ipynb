{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e741647-eb75-496a-ae0f-0d629f00528e",
   "metadata": {},
   "source": [
    "В следующих заданиях будет использоваться датасет boston из sklearn.datasets.\n",
    "\n",
    "Оставьте последние 25% объектов для контроля качества, разделив X и y на X_train, y_train и X_test, y_test с помощью train_test_split(X, y, train_size = 0.75, shuffle=False).\n",
    "\n",
    "Целью заданий будет реализовать простой вариант градиентного бустинга над регрессионными деревьями для случая квадратичной функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1cc1fda6-49fd-46a7-a04d-5041ee674cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f67038b5-6278-4031-ae0f-ab8f6c6baf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a4e9f53-5e4b-43b7-80f7-fe2d4866595a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dex/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "boston = datasets.load_boston()\n",
    "X, y = boston.data, boston.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.75, shuffle=False) #train_test_split(X, y, test_size=0.25, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c7640f-babb-4afa-9115-0b3435c061b7",
   "metadata": {},
   "source": [
    "#### Задание 1\n",
    "\n",
    "Как вы уже знаете из лекций, бустинг - это метод построения композиций базовых алгоритмов с помощью последовательного добавления к текущей композиции нового алгоритма с некоторым коэффициентом.\n",
    "\n",
    "Градиентный бустинг обучает каждый новый алгоритм так, чтобы он приближал антиградиент ошибки по ответам композиции на обучающей выборке. Аналогично минимизации функций методом градиентного спуска, в градиентном бустинге мы подправляем композицию, изменяя алгоритм в направлении антиградиента ошибки.\n",
    "\n",
    "Воспользуйтесь формулой из лекций, задающей ответы на обучающей выборке, на которые нужно обучать новый алгоритм (фактически это лишь чуть более подробно расписанный градиент от ошибки), и получите частный ее случай, если функция потерь L - квадрат отклонения ответа композиции a(x) от правильного ответа y на данном x.\n",
    "\n",
    "Если вы давно не считали производную самостоятельно, вам поможет таблица производных элементарных функций (которую несложно найти в интернете) и правило дифференцирования сложной функции. После дифференцирования квадрата у вас возникнет множитель 2 — т.к. нам все равно предстоит выбирать коэффициент, с которым будет добавлен новый базовый алгоритм, проигноируйте этот множитель при дальнейшем построении алгоритма.\n",
    "\n",
    "Запишите в качестве ответа формулу производной без константы 2.\n",
    "\n",
    "Для ввода формулы НЕ используйте пробел, символ дифференцирования обозначьте одинарной кавычкой (он же штрих), а производная должна быть последним членом при умножении. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93101848-1a4f-4d4e-986b-763a78d0de7a",
   "metadata": {},
   "source": [
    "(y-a(x))*a'(x)  is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b1210ac-c61f-47ec-ac49-0620a6a35363",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задание 2\n",
    "\n",
    "Заведите массив для объектов __DecisionTreeRegressor__ (будем их использовать в качестве базовых алгоритмов) и для вещественных чисел (это будут коэффициенты перед базовыми алгоритмами).\n",
    "\n",
    "В цикле обучите последовательно 50 решающих деревьев с параметрами max_depth=5 и random_state=42 (остальные параметры - по умолчанию). В бустинге зачастую используются сотни и тысячи деревьев, но мы ограничимся 50, чтобы алгоритм работал быстрее, и его было проще отлаживать (т.к. цель задания разобраться, как работает метод). Каждое дерево должно обучаться на одном и том же множестве объектов, но ответы, которые учится прогнозировать дерево, будут меняться в соответствие с полученным в задании 1 правилом.\n",
    "\n",
    "Попробуйте для начала всегда брать коэффициент равным 0.9. Обычно оправдано выбирать коэффициент значительно меньшим - порядка 0.05 или 0.1, но т.к. в нашем учебном примере на стандартном датасете будет всего 50 деревьев, возьмем для начала шаг побольше.\n",
    "\n",
    "В процессе реализации обучения вам потребуется функция, которая будет вычислять прогноз построенной на данный момент композиции деревьев на выборке X:\n",
    "\n",
    "    def gbm_predict(X): return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X] (считаем, что base_algorithms_list - список с базовыми алгоритмами, coefficients_list - список с коэффициентами перед алгоритмами)\n",
    "\n",
    "Эта же функция поможет вам получить прогноз на контрольной выборке и оценить качество работы вашего алгоритма с помощью __mean_squared_error__ в sklearn.metrics.\n",
    "\n",
    "Возведите результат в степень 0.5, чтобы получить RMSE. Полученное значение RMSE введите в поле для ответа:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bff64c38-70d8-4e30-ad36-f1476f435b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#функция, которая будет вычислять прогноз построенной на данный момент композиции деревьев на выборке X:\n",
    "def gbm_predict(X): return [sum([coeff * algo.predict([x])[0] for algo, coeff in zip(base_algorithms_list, coefficients_list)]) for x in X]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590961f6-afb6-4c88-8082-2be671eef347",
   "metadata": {},
   "source": [
    "В цикле используем x_train и y_train. В конце, где считаем корень, надо будет брать y_test и предсказание от x_test."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8796fc-e2a4-4032-b3d7-1f648825e051",
   "metadata": {},
   "source": [
    "(a)- мы вычисляем новые y для обучения следующего дерева с помощью кода \n",
    "    y_pred = y_train - gbm_predict(X_train) \n",
    "что по идее эквивалентно (y-a(x)) из формулы в предыдущем задании. Но ведь у нас еще есть a'(x), что с ней? \n",
    "\n",
    "(b)- в задании 6.4.2 применяется лишь общий подход, а не частный случай, описанный в 6.4.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c72118ef-b40b-4649-9572-bf9d4c601fc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3174600019002005\n"
     ]
    }
   ],
   "source": [
    "base_algorithms_list = [] # массив для объектов DecisionTreeRegressor (будем их использовать в качестве базовых алгоритмов)\n",
    "coefficients_list = [] # массив для вещественных чисел (это будут коэффициенты перед базовыми алгоритмами)\n",
    "\n",
    "alpha = 0.9\n",
    "MAX_DEPTH = 5\n",
    "RANDOM_SEED = 139\n",
    "ITERATIONS = 50\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    model = DecisionTreeRegressor(max_depth = MAX_DEPTH, random_state = RANDOM_SEED)\n",
    "    y_pred = y_train - gbm_predict(X_train)\n",
    "\n",
    "    model.fit(X_train, y_pred)\n",
    "    \n",
    "    base_algorithms_list.append(model)\n",
    "    alpha = 0.9 # 0.9 / (1.0 + i)\n",
    "    coefficients_list.append(alpha)\n",
    "    \n",
    "RMSE = mean_squared_error(y_true = y_test, y_pred = gbm_predict(X_test))**0.5\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25fdbfba-fba6-49ba-80bc-8c8a41fc3e98",
   "metadata": {},
   "source": [
    "5.317530 is correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "480a698f-f241-46f3-82d7-2f2b02888623",
   "metadata": {},
   "source": [
    "RMSE : Ошибка должна уменьшаться на обучающей выборке - на тестовой она может и расти (это переобучение)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5184230d-7b45-4227-9317-3afc21803e89",
   "metadata": {},
   "source": [
    "(b)- на первой итерации базовый алгоритм должен учиться непосредственно на y_train, а далее уже на разнице между y_train и y_pred, т.е.:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a770a906-7c3c-46a5-8320-bb216ac5eeb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.3174600019002005\n"
     ]
    }
   ],
   "source": [
    "base_algorithms_list = [] # массив для объектов DecisionTreeRegressor (будем их использовать в качестве базовых алгоритмов)\n",
    "coefficients_list = [] # массив для вещественных чисел (это будут коэффициенты перед базовыми алгоритмами)\n",
    "\n",
    "s = y_train.copy()\n",
    "for i in range(ITERATIONS):\n",
    "    model = DecisionTreeRegressor(max_depth = MAX_DEPTH, random_state = RANDOM_SEED)\n",
    "    model.fit(X_train, s)\n",
    "    base_algorithms_list.append(model)\n",
    "    coefficients_list.append(alpha)\n",
    "    \n",
    "    y_pred = gbm_predict(X_train)\n",
    "    s = (y_train - y_pred)\n",
    "    \n",
    "    coefficients_list.append(alpha)\n",
    "    \n",
    "RMSE = mean_squared_error(y_true = y_test, y_pred = gbm_predict(X_test))**0.5\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9770236-6113-41a9-9819-45e637acc93b",
   "metadata": {},
   "source": [
    "#### Задание 3\n",
    "\n",
    "Вас может также беспокоить, что двигаясь с постоянным шагом, вблизи минимума ошибки ответы на обучающей выборке меняются слишком резко, перескакивая через минимум.\n",
    "\n",
    "Попробуйте уменьшать вес перед каждым алгоритмом с каждой следующей итерацией по формуле 0.9 / (1.0 + i), где i - номер итерации (от 0 до 49). Используйте качество работы алгоритма как ответ в пункте 3.\n",
    "\n",
    "В реальности часто применяется следующая стратегия выбора шага: как только выбран алгоритм, подберем коэффициент перед ним численным методом оптимизации таким образом, чтобы отклонение от правильных ответов было минимальным. Мы не будем предлагать вам реализовать это для выполнения задания, но рекомендуем попробовать разобраться с такой стратегией и реализовать ее при случае для себя.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "2c35445a-2726-46fb-adac-f04826c240ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.88907239698691\n"
     ]
    }
   ],
   "source": [
    "base_algorithms_list = [] # массив для объектов DecisionTreeRegressor (будем их использовать в качестве базовых алгоритмов)\n",
    "coefficients_list = [] # массив для вещественных чисел (это будут коэффициенты перед базовыми алгоритмами)\n",
    "\n",
    "s = y_train.copy()\n",
    "for i in range(ITERATIONS):\n",
    "    model = DecisionTreeRegressor(max_depth = MAX_DEPTH, random_state = RANDOM_SEED)\n",
    "    model.fit(X_train, s)\n",
    "    base_algorithms_list.append(model)\n",
    "    alpha = 0.9 / (1.0 + i)\n",
    "    coefficients_list.append(alpha)\n",
    "    \n",
    "    y_pred = gbm_predict(X_train)\n",
    "    s = (y_train - y_pred)\n",
    "    \n",
    "    coefficients_list.append(alpha)\n",
    "    \n",
    "RMSE = mean_squared_error(y_true = y_test, y_pred = gbm_predict(X_test))**0.5\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404dd87a-3505-4cdd-a78e-dec57a283e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.9\n",
    "MAX_DEPTH = 5\n",
    "RANDOM_SEED = 139\n",
    "ITERATIONS = 50\n",
    "\n",
    "for i in range(ITERATIONS):\n",
    "    model = DecisionTreeRegressor(max_depth = MAX_DEPTH, random_state = RANDOM_SEED)\n",
    "    y_pred = y_train - gbm_predict(X_train)\n",
    "\n",
    "    model.fit(X_train, y_pred)\n",
    "    \n",
    "    base_algorithms_list.append(model)\n",
    "    alpha = 0.9 / (1.0 + i)\n",
    "    coefficients_list.append(alpha)\n",
    "    \n",
    "RMSE = mean_squared_error(y_true = y_test, y_pred = gbm_predict(X_test))**0.5\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65f645ac-1040-46fb-9700-1befe0abfd04",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задание 4\n",
    "\n",
    "Реализованный вами метод - градиентный бустинг над деревьями - очень популярен в машинном обучении. Он представлен как в самой библиотеке sklearn, так и в сторонней библиотеке XGBoost, которая имеет свой питоновский интерфейс. На практике XGBoost работает заметно лучше GradientBoostingRegressor из sklearn, но для этого задания вы можете использовать любую реализацию.\n",
    "\n",
    "Исследуйте, переобучается ли градиентный бустинг с ростом числа итераций (и подумайте, почему), а также с ростом глубины деревьев. На основе наблюдений выпишите через пробел номера правильных из приведенных ниже утверждений в порядке возрастания номера (это будет ответ в п.4):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3316f54e-9f69-402a-96ff-3d9cecf4a5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "%pylab inline\n",
    "\n",
    "trees = np.array([5, 10, 15, 20, 30, 40, 50, 60, 70, 100])\n",
    "depths = np.array([3, 5, 7, 10, 15, 22])\n",
    "pyplot.figure(figsize=(16, 11))\n",
    "i = 1\n",
    "for d in depths:\n",
    "    scores_train, scores_test = list(), list()\n",
    "    for num in trees:\n",
    "        boost = GradientBoostingRegressor(n_estimators=num, max_depth=d, random_state=42).fit(X_train, y_train)\n",
    "        scores_train.append(np.sqrt(mean_squared_error(y_train, boost.predict(X_train))))\n",
    "        scores_test.append(np.sqrt(mean_squared_error(y_test, boost.predict(X_test))))\n",
    "    print(scores_test[:3])\n",
    "    ax = plt.subplot(3, 3, i)\n",
    "    ax.set_title(\"forest {} depth\".format(d))\n",
    "    pyplot.plot(trees, scores_train, color=\"red\", label=\"train\")\n",
    "    pyplot.plot(trees, scores_test, color=\"blue\", label=\"test\")\n",
    "    pyplot.xlabel(\"trees\")\n",
    "    pyplot.ylabel(\"score\")\n",
    "    pyplot.legend()\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c396afe4-4721-4e70-b615-9bd97be2b895",
   "metadata": {},
   "source": [
    "#### Задание 5\n",
    "\n",
    "Сравните получаемое с помощью градиентного бустинга качество с качеством работы линейной регрессии.\n",
    "\n",
    "Для этого обучите LinearRegression из sklearn.linear_model (с параметрами по умолчанию) на обучающей выборке и оцените для прогнозов полученного алгоритма на тестовой выборке RMSE. Полученное качество - ответ в пункте 5.\n",
    "\n",
    "В данном примере качество работы простой модели должно было оказаться хуже, но не стоит забывать, что так бывает не всегда. В заданиях к этому курсу вы еще встретите пример обратной ситуации.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d3c0f-011f-418c-95ab-488fa20a6f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X_train,y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "RMSE = print_regression_metrics(y_test, y_pred)**0.5 #error\n",
    "print(RMSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d7bf3-b75b-4306-bb9e-0cf2421fde8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
