{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48c0dac0-bcb2-4805-bb82-1e73d9745e72",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 9.4 Строить ассоциативные правила мы будем на основе датасета от Netflix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b9b3e8-4d82-4c25-8bbb-974c85b5e7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data_fin.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1339583-64dd-4be7-9d49-1a563269278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc6a92-d3f9-4b69-aa5c-861d6db310c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Из полученного датасета возьмём только те записи, у которых наивысший рейтинг (5) и объединим их по \"Cust_Id\". \n",
    "#Фильмы сгруппируем в строчку с разделителем \"пробел\" так, чтобы для каждого пользователя была строка с ID тех фильмов, которые ему понравились:\n",
    "good = df[df['Rating']==5].groupby('Cust_Id')['Movie_Id'].apply(lambda r: ' '.join([str(A) for A in r]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac3b9cef-7c14-47fb-a677-6ca7cdd45889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import apyori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a17e2f-450c-45d3-8d3a-a3c74fc4d23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Теперь, когда необходимая библиотека подгружена, сделаем несколько ассоциативных правил. \n",
    "#Мы можем регулировать их количество, меняя параметры алгоритмов. Посмотрим, какие ассоциативные правила получаются для support = 0.04\n",
    "association_rules = apyori.apriori(good.apply(lambda r: r.split(' ')), \n",
    "                                   min_support=0.04, \n",
    "                                   min_confidence=0.1, min_lift=2, \n",
    "                                   min_length=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "18623178-1e72-49da-93eb-1be1f80be551",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'association_rules' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_10798/3449874435.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0massociation_rules\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'association_rules' is not defined"
     ]
    }
   ],
   "source": [
    "association_rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23a2463-91ec-43a7-ac2f-f05f92125ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пройдёмся по генератору и объединим его результаты. \n",
    "asr_df = pd.DataFrame(columns = ['from', 'to', 'confidence', 'support', 'lift'])\n",
    "for item in association_rules:\n",
    "    pair = item[0] \n",
    "    items = [x for x in pair]\n",
    "    asr_df.loc[len(asr_df), :] =  ' '.join(list(item[2][0][0])), \\\n",
    "                                  ' '.join(list(item[2][0][1])),\\\n",
    "                                  item[2][0][2], item[1], item[2][0][3]\n",
    "\n",
    "    \n",
    "asr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa850cb3-c641-40de-8cc3-72ba2bb8a492",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Для того чтобы перейти от ID фильмов к их названиям, нужно загрузить ещё один файл, в котором содержится ID фильма, год его производства и название:\n",
    "titles = pd.read_csv('movie_titles.csv', encoding = \"ISO-8859-1\", \n",
    "                     header = None, \n",
    "                     names = ['Movie_Id', 'Year', 'Name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56aa126a-5e17-4b4d-847c-b7661a0eecd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Мы можем написать процедуру, которая будет выводить названия фильмов в ассоциативном правиле и фильм, который это ассоциативное правило рекомендует:\n",
    "def get_rule_title(rule):\n",
    "    print(titles[titles.Movie_Id.isin(rule['from'].split(' '))]['Name'].values)\n",
    "    print('----------->')\n",
    "    #9.4 в ноутбуке ошибка:\n",
    "    # print(titles[titles.Movie_Id == int(rule['to'])]['Name'].values)\n",
    "#Третий print в случае если список to из > 1 фильма не работает. Так работает:    \n",
    "    print(titles[titles.Movie_Id.isin(rule['to'].split(' '))]['Name'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9755fde5-f3d1-4c9c-9e9d-fff6e9529fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rule_title(asr_df.loc[99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342c4875-0bb4-48ed-a131-6ce0ebdcc964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Перейдём к построению рекомендаций для случайного человека под ID=159992. Посмотрим, какие фильмы он смотрел и как он их оценил. \n",
    "j = 159992\n",
    "titles[titles.Movie_Id.isin(good.iloc[j].split(' '))]['Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86621ae6-6349-4c1c-a1f6-23ec2e72282f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Как мы можем посчитать рекомендации для этого человека? Мы можем пройтись по всем правилам в нашей таблице и проверить: \n",
    "#если они присутствуют в просмотрах человека и у них высокий рейтинг, значит это правило ему подходит и мы можем добавить этот фильм в список рекомендаций.\n",
    "def print_rule_title(rule):\n",
    "    return (titles[titles.Movie_Id == int(rule['to'])]['Name'].values)\n",
    "    \n",
    "\n",
    "result = []\n",
    "for A in asr_df.index:\n",
    "    if len(set(good.iloc[j].split(' ')) & set(asr_df['from'].loc[A].split(' '))) == len(asr_df['from'].loc[A].split(' ')):\n",
    "        result.append(print_rule_title(asr_df.loc[A])[0])\n",
    "print(set(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a49ab2c-9889-417c-9145-814bbf7543aa",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# 9.6 Продолжим работать с датасетом Netflix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897973de-ddcf-4c9d-945a-f5b5319bae7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Возьмём подвыборку из 10000 случайных кастомеров и 5000 фильмов. \n",
    "cust_sample = df.Cust_Id.sample(10000)\n",
    "movie_sample = df.Movie_Id.sample(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcce2ab6-6a2f-4536-83e1-0cf89565bf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Для генерации простых рекомендаций с помощью коллаборативной фильтрации можно воспользоваться модулем surprise. Загрузим в модуль surprise наш датасет с помощью метода Reader. \n",
    "\n",
    "Предварительно необходимо установить модуль surprise, он не является предустановленным.\n",
    "\n",
    "Это можно сделать через pip. В случае, если это не работает, можно воспользоваться одним из следующих четырёх вариантов:\n",
    "\n",
    "conda install -c conda-forge scikit-surprise\n",
    "conda install -c conda-forge/label/gcc7 scikit-surprise\n",
    "conda install -c conda-forge/label/cf201901 scikit-surprise\n",
    "conda install -c conda-forge/label/cf202003 scikit-surprise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2420c10d-b79e-4e13-96c9-0f0cea025ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import surprise\n",
    "from surprise import Reader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64aac5f6-0530-42ef-8218-baee1678676d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Возьмём только те оценки, которые относятся к выбранному подмножеству кастомеров, и только те оценки, которые относятся к выбранному подмножеству фильмов. \n",
    "#Именно в такой последовательности — сначала Cust_Id, затем Movie_Id, затем Rating.\n",
    "reader = Reader(rating_scale=(0.5, 5.0))\n",
    "data = Dataset.load_from_df(df[df.Cust_Id.isin(cust_sample) &\n",
    "                              df.Movie_Id.isin(movie_sample)][['Cust_Id', 'Movie_Id', 'Rating']], reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d9b6f1-ee20-47f1-b286-105b062a9b2b",
   "metadata": {},
   "source": [
    "\n",
    "В модуле surprise есть несколько реализаций коллаборативной фильтрации. Мы возьмём одну из самых самых простых — принцип ближайших соседей.\n",
    "\n",
    "Принцип коллаборативной фильтрации заключается в следующем:\n",
    "\n",
    "Для каждого человека находится небольшое множество похожих на него зрителей с оценками примерно такими же, какие поставил человек на ряд фильмов (item). Из этой группы можно усреднить оценки на просмотренные фильмы и для тех членов группы, у которых ещё не было просмотров этих фильмов, экстраполировать значения оценок в этих ячейках.\n",
    "\n",
    "Таким образом, у нас появляется некая средняя оценка в группе для каждого фильма из просмотренных, и мы можем предположить, что тем людям, которые ещё не успели посмотреть эти фильмы, они понравятся.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634f98c6-a4bb-493f-b704-658c875baad1",
   "metadata": {},
   "source": [
    "Так как размерность по пользователям больше, чем размерность по фильмам, то выгоднее использовать не user-based алгоритм, а item-based. В этом случае вектор будет состоять не из оценок одного пользователя на различные фильмы, а будет содержать все оценки фильма от многих пользователей. Таким образом мы получим больший вектор, но само количество векторов будет меньше. А если меньше количество векторов, то проще посчитать матрицу из взаимной дистанции.\n",
    "\n",
    "Именно это мы задаём в качестве параметров алгоритма:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7abc2b2-b530-4016-b621-eb2061dd1b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic\n",
    "\n",
    "sim_options = {\n",
    "    'name': 'cosine',\n",
    "    'user_based': False\n",
    "}\n",
    "\n",
    "#Запускаем алгоритм и формирует датасет для тренировки специальной функцией build_full_trainset().\n",
    "knn = KNNBasic(sim_options=sim_options)\n",
    "trainingSet = data.build_full_trainset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4790cd0-44d6-4e71-83ba-b928f26db931",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'knn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3523/3556476189.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#После этого проводим тренировку модели на сформированном тренировочном датасете:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mknn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainingSet\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'knn' is not defined"
     ]
    }
   ],
   "source": [
    "#После этого проводим тренировку модели на сформированном тренировочном датасете:\n",
    "\n",
    "knn.fit(trainingSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e8f9b1e-ac17-4eb7-aaca-524071f48899",
   "metadata": {},
   "outputs": [],
   "source": [
    "#С помощью натренированной модели мы можем проскорить остальные оценки. Для этого сгенерируем тестовый сет и построим предсказание по этому датасету:\n",
    "\n",
    "testSet = trainingSet.build_anti_testset()\n",
    "predictions = knn.test(testSet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4b3b1-92a3-4117-b613-4bea3bae24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Результат получился неудобочитаемым. Поэтому сделаем вспомогательную функцию, которая будет брать топ-3 фильмов и их оценки:\n",
    "\n",
    "from collections import defaultdict\n",
    " \n",
    "def get_top3_recommendations(predictions, topN = 3):\n",
    "     \n",
    "    top_recs = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_recs[uid].append((iid, est))\n",
    "     \n",
    "    for uid, user_ratings in top_recs.items():\n",
    "        user_ratings.sort(key = lambda x: x[1], reverse = True)\n",
    "        top_recs[uid] = user_ratings[:topN]\n",
    "     \n",
    "    return top_recs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2307354-1ce7-4323-be2f-d17da8536342",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Обрабатываем наше предсказание:\n",
    "\n",
    "top3_recommendations = get_top3_recommendations(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8607eb8c-5834-40d0-8965-2768bbf45563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#С помощью следующей функции переведём тексты фильмов в удобочитаемый вид, то есть раскодируем заглавия фильмов. \n",
    "\n",
    "import numpy as np\n",
    "def print_recs(i):\n",
    "    for (a, b) in top3_recommendations[i]:\n",
    "        print(titles[titles.Movie_Id == a]['Name'].values[0], np.round(b,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9554bc31-7c6f-4a17-9972-9798ad4f3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# С помощью этой функции выведем рекомендации для случайного пользователя:\n",
    "\n",
    "i = np.random.choice(list(top3_recommendations.keys()))\n",
    "print_recs(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928c13d6-b6c2-4a48-9238-2c385d5302ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим, что смотрел этот человек, и выберем из нашего датасета те фильмы, которые этот человек оценил на 5. \n",
    "\n",
    "films = data.df[(data.df.Cust_Id == i) & (data.df.Rating == 5)]['Movie_Id'].values\n",
    "titles[titles.Movie_Id.isin(films)]['Name'].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
