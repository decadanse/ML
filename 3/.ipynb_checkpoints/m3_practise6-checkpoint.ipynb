{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error, f1_score, accuracy_score, roc_curve, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.Когда использовать матричные операции вместо градиентного спуска в линейной регрессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_regression_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    print(f'MSE = {mse:.2f}, RMSE = {rmse:.2f}')\n",
    "    \n",
    "def prepare_boston_data():\n",
    "    data = load_boston()\n",
    "    X, y = data['data'], data['target']\n",
    "    # Нормализовать даннные с помощью стандартной нормализации\n",
    "    X = (X - X.mean(axis=0)) / X.std(axis=0)\n",
    "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем начать, обернем написанную нами линейную регрессию методом матричных операций в класс:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinRegAlgebra():\n",
    "    def __init__(self):\n",
    "        self.theta = None\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        self.theta = np.linalg.inv(X.transpose().dot(X)).dot(X.transpose()).dot(y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return X.dot(self.theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведем замеры скорости работы алгоритмов на матричных операциях и на градиентном спуске. Предварительно найдем параметры для метода, основанного на градиентном спуске, так, чтобы значения метрик максимально совпадало со значениями в случае первого алгоритма."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dex/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "X, y = prepare_boston_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 21.89, RMSE = 4.68\n"
     ]
    }
   ],
   "source": [
    "linreg_alg = LinRegAlgebra()\n",
    "linreg_alg.fit(X, y)\n",
    "y_pred = linreg_alg.predict(X)\n",
    "\n",
    "# Посчитать значение ошибок MSE и RMSE для тренировочных данных\n",
    "print_regression_metrics(y, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.6.1\n",
    "Сделайте для градиентного спуска остановку алгоритма, если максимальное из абсолютных значений компонент градиента становится меньше 0.01. \n",
    "Сравните скорость обучения градиентным спуском и матричными операциями. \n",
    "Для градиентного спуска установите alpha = 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegOptimizer():\n",
    "    def __init__(self, alpha, n_iters):\n",
    "        self.theta = None\n",
    "        self._alpha = alpha\n",
    "        self._n_iters = n_iters\n",
    "    \n",
    "    def gradient_step(self, theta, theta_grad):\n",
    "        \n",
    "        return theta - self._alpha * theta_grad \n",
    "     \n",
    "    \n",
    "    def grad_func(self, X, y, theta):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def optimize(self, X, y, start_theta, n_iters):\n",
    "        theta = start_theta.copy()\n",
    "        theta_mas=[]\n",
    "        for i in range(n_iters):\n",
    "            theta_grad = self.grad_func(X, y, theta)\n",
    "            theta = self.gradient_step(theta, theta_grad)\n",
    "           \n",
    "        ###3.6.1\n",
    "            if np.max(abs(theta_grad))<=0.01:\n",
    "                print (i) #на этой итерации останавливается градиентный спуск\n",
    "                break\n",
    "        ###3.6.1\n",
    "        \n",
    "        return theta     \n",
    "                    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        m = X.shape[1]\n",
    "        start_theta = np.ones(m)\n",
    "        self.theta = self.optimize(X, y, start_theta, self._n_iters)\n",
    "        \n",
    "    def predict(self, X):\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinReg(RegOptimizer):\n",
    "    def grad_func(self, X, y, theta):\n",
    "        n = X.shape[0]\n",
    "        grad = 1. / n * X.transpose().dot(X.dot(theta) - y)\n",
    "        \n",
    "        \n",
    "        return grad\n",
    "    \n",
    "    def predict(self, X):\n",
    "        if self.theta is None:\n",
    "            raise Exception('You should train the model first')\n",
    "        \n",
    "        y_pred = X.dot(self.theta)\n",
    "        \n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.6.1\n",
    "Сделайте для градиентного спуска остановку алгоритма, если максимальное из абсолютных значений компонент градиента становится меньше **0.01**. Сравните скорость обучения градиентным спуском и матричными операциями.\n",
    "\n",
    "На какой итерации останавливается градиентный спуск?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "MSE = 21.90, RMSE = 4.68\n"
     ]
    }
   ],
   "source": [
    "#fig,ax = plt.subplots(figsize=(16,8))\n",
    "linreg_crit = LinReg(0.2,1000)\n",
    "linreg_crit.fit(X, y)\n",
    "y_pred = linreg_crit.predict(X)\n",
    "\n",
    "# Посчитать значение ошибок MSE и RMSE для тренировочных данных\n",
    "print_regression_metrics(y, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The slowest run took 4.35 times longer than the fastest. This could mean that an intermediate result is being cached.\n",
      "99.5 µs ± 72.1 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit linreg_alg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "212\n",
      "5.3 ms ± 65.8 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit linreg_crit.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "212\n"
     ]
    }
   ],
   "source": [
    "\n",
    "linreg_crit.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из результатов эксперимента, реализация на матричных операциях опережает реализацию на градиентном спуске в 500 раз. Но всегда ли это так и какие подводные камни могут быть? Ниже приведен набор случаев, при которых версия с градентным спуском предпочтительнее:\n",
    "\n",
    "1. Градиентный спуск работает быстрее в случае матриц с большим количеством признаков. Основная по сложности операция — нахождение обратной матрицы $(X^T X)^{-1}$.\n",
    "1. Нахождение обратной матрицы может также потребовать больше оперативной памяти, что иногда является не приемлемым.\n",
    "1. Матричные операции могут также проигрывать и в случае небольших объемов данных, но при плохой параллельной реализации или недостаточных ресурсах.\n",
    "1. Градиентный спуск может быть усовершенствован до так называемого **стохастического градиентного спуска**, при котором данные для оптимизации подгружаются небольшими наборами, что уменьшает требования по памяти.\n",
    "1. В некоторых случаях (например, в случае линейно-зависимых строк) алгебраический способ решения не будет работать совсем в виду невозможности найти обратную матрицу."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Превращение линейной модели в нелинейную"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Нелинейные зависимости в данных встречаются намного чаще линейных. На самом деле простейшая линейная регрессия способна обнаруживать нелинейные зависимости. Для этого необходимо рассмотреть дополнительные признаки, полученные из исходных применением различных нелинейных функций. Возьмем уже знакомый датасет с ценами на квартиры в Бостоне и последовательно станем применять различные функции к исходным признакам:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boston Data. Attribute Information (in order):\n",
    "    - CRIM     per capita crime rate by town\n",
    "    - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
    "    - INDUS    proportion of non-retail business acres per town\n",
    "    - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
    "    - NOX      nitric oxides concentration (parts per 10 million)\n",
    "    - RM       average number of rooms per dwelling\n",
    "    - AGE      proportion of owner-occupied units built prior to 1940\n",
    "    - DIS      weighted distances to five Boston employment centres\n",
    "    - RAD      index of accessibility to radial highways\n",
    "    - TAX      full-value property-tax rate per `$10000`\n",
    "    - PTRATIO  pupil-teacher ratio by town\n",
    "    - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
    "    - LSTAT    % lower status of the population\n",
    "    - MEDV     Median value of owner-occupied homes in $1000's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX',\n",
       "       'PTRATIO', 'B', 'LSTAT'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_boston()\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.09  ,  4.9671,  4.9671,  6.0622,  6.0622,  6.0622,  5.5605,\n",
       "        5.9505,  6.0821,  6.5921,  6.3467,  6.2267,  5.4509,  4.7075,\n",
       "        4.4619,  4.4986,  4.4986,  4.2579,  3.7965,  3.7965,  3.7979,\n",
       "        4.0123,  3.9769,  4.0952,  4.3996,  4.4546,  4.682 ,  4.4534,\n",
       "        4.4547,  4.239 ,  4.233 ,  4.175 ,  3.99  ,  3.7872,  3.7598,\n",
       "        3.3603,  3.3779,  3.9342,  3.8473,  5.4011,  5.4011,  5.7209,\n",
       "        5.7209,  5.7209,  5.7209,  5.1004,  5.1004,  5.6894,  5.87  ,\n",
       "        6.0877,  6.8147,  6.8147,  6.8147,  6.8147,  7.3197,  8.6966,\n",
       "        9.1876,  8.3248,  7.8148,  6.932 ,  7.2254,  6.8185,  7.2255,\n",
       "        7.9809,  9.2229,  6.6115,  6.6115,  6.498 ,  6.498 ,  6.498 ,\n",
       "        5.2873,  5.2873,  5.2873,  5.2873,  4.2515,  4.5026,  4.0522,\n",
       "        4.0905,  5.0141,  4.5026,  5.4007,  5.4007,  5.4007,  5.4007,\n",
       "        4.7794,  4.4377,  4.4272,  3.7476,  3.4217,  3.4145,  3.0923,\n",
       "        3.0921,  3.6659,  3.6659,  3.615 ,  3.4952,  3.4952,  3.4952,\n",
       "        3.4952,  3.4952,  2.7778,  2.8561,  2.7147,  2.7147,  2.421 ,\n",
       "        2.1069,  2.211 ,  2.1224,  2.4329,  2.5451,  2.7778,  2.6775,\n",
       "        2.3534,  2.548 ,  2.2565,  2.4631,  2.7301,  2.7474,  2.4775,\n",
       "        2.7592,  2.2577,  2.1974,  2.0869,  1.9444,  2.0063,  1.9929,\n",
       "        1.7572,  1.7883,  1.8125,  1.9799,  2.1185,  2.271 ,  2.3274,\n",
       "        2.4699,  2.346 ,  2.1107,  1.9669,  1.8498,  1.6686,  1.6687,\n",
       "        1.6119,  1.4394,  1.3216,  1.4118,  1.3459,  1.4191,  1.5166,\n",
       "        1.4608,  1.5296,  1.5257,  1.618 ,  1.5916,  1.6102,  1.6232,\n",
       "        1.7494,  1.7455,  1.7364,  1.8773,  1.7573,  1.7659,  1.7984,\n",
       "        1.9709,  2.0407,  2.162 ,  2.422 ,  2.2834,  2.0459,  2.4259,\n",
       "        2.1   ,  2.2625,  2.4259,  2.3887,  2.5961,  2.6463,  2.7019,\n",
       "        3.1323,  3.5549,  3.3175,  2.9153,  2.829 ,  2.741 ,  2.5979,\n",
       "        2.7006,  2.847 ,  2.9879,  3.2797,  3.1992,  3.7886,  4.5667,\n",
       "        4.5667,  6.4798,  6.4798,  6.4798,  6.2196,  6.2196,  5.6484,\n",
       "        7.309 ,  7.309 ,  7.309 ,  7.6534,  7.6534,  6.27  ,  6.27  ,\n",
       "        5.118 ,  5.118 ,  3.9454,  4.3549,  4.3549,  4.2392,  3.875 ,\n",
       "        3.8771,  3.665 ,  3.6526,  3.9454,  3.5875,  3.9454,  3.1121,\n",
       "        3.4211,  2.8893,  3.3633,  2.8617,  3.048 ,  3.2721,  3.2721,\n",
       "        2.8944,  2.8944,  3.2157,  3.2157,  3.3751,  3.3751,  3.6715,\n",
       "        3.6715,  3.8384,  3.6519,  3.6519,  3.6519,  4.148 ,  4.148 ,\n",
       "        6.1899,  6.1899,  6.3361,  6.3361,  7.0355,  7.0355,  7.9549,\n",
       "        7.9549,  8.0555,  8.0555,  7.8265,  7.8265,  7.3967,  7.3967,\n",
       "        8.9067,  8.9067,  9.2203,  9.2203,  6.3361,  1.801 ,  1.8946,\n",
       "        2.0107,  2.1121,  2.1398,  2.2885,  2.0788,  1.9301,  1.9865,\n",
       "        2.1329,  2.4216,  2.872 ,  3.9175,  4.429 ,  4.429 ,  3.9175,\n",
       "        4.3665,  4.0776,  4.2673,  4.7872,  4.8628,  4.1403,  4.1007,\n",
       "        4.6947,  5.2447,  5.2119,  5.885 ,  7.3073,  7.3073,  9.0892,\n",
       "        7.3172,  7.3172,  7.3172,  5.1167,  5.1167,  5.1167,  5.5027,\n",
       "        5.5027,  5.9604,  5.9604,  6.32  ,  7.8278,  7.8278,  7.8278,\n",
       "        5.4917,  5.4917,  5.4917,  4.022 ,  3.37  ,  3.0992,  3.1827,\n",
       "        3.3175,  3.1025,  2.5194,  2.6403,  2.834 ,  3.2628,  3.6023,\n",
       "        3.945 ,  3.9986,  4.0317,  3.5325,  4.0019,  4.5404,  4.5404,\n",
       "        4.7211,  4.7211,  4.7211,  5.4159,  5.4159,  5.4159,  5.2146,\n",
       "        5.2146,  5.8736,  6.6407,  6.6407,  6.4584,  6.4584,  5.9853,\n",
       "        5.2311,  5.615 ,  4.8122,  4.8122,  4.8122,  7.0379,  6.2669,\n",
       "        5.7321,  6.4654,  8.0136,  8.0136,  8.5353,  8.344 ,  8.7921,\n",
       "        8.7921, 10.7103, 10.7103, 12.1265, 10.5857, 10.5857,  2.1222,\n",
       "        2.5052,  2.7227,  2.5091,  2.5182,  2.2955,  2.1036,  1.9047,\n",
       "        1.9047,  1.6132,  1.7523,  1.5106,  1.3325,  1.3567,  1.2024,\n",
       "        1.1691,  1.1296,  1.1742,  1.137 ,  1.3163,  1.3449,  1.358 ,\n",
       "        1.3861,  1.3861,  1.4165,  1.5192,  1.5804,  1.5331,  1.4395,\n",
       "        1.4261,  1.4672,  1.5184,  1.5895,  1.7281,  1.9265,  2.1678,\n",
       "        1.77  ,  1.7912,  1.7821,  1.7257,  1.6768,  1.6334,  1.4896,\n",
       "        1.5004,  1.5888,  1.5741,  1.639 ,  1.7028,  1.6074,  1.4254,\n",
       "        1.1781,  1.2852,  1.4547,  1.4655,  1.413 ,  1.5275,  1.5539,\n",
       "        1.5894,  1.6582,  1.8347,  1.8195,  1.6475,  1.8026,  1.794 ,\n",
       "        1.8589,  1.8746,  1.9512,  2.0218,  2.0635,  1.9096,  1.9976,\n",
       "        1.8629,  1.9356,  1.9682,  2.0527,  2.0882,  2.2004,  2.3158,\n",
       "        2.2222,  2.1247,  2.0026,  1.9142,  1.8206,  1.8172,  1.8662,\n",
       "        2.0651,  2.0048,  1.9784,  1.8956,  1.9879,  2.072 ,  2.198 ,\n",
       "        2.2616,  2.185 ,  2.3236,  2.3552,  2.3682,  2.4527,  2.4961,\n",
       "        2.4358,  2.5806,  2.7792,  2.7831,  2.7175,  2.5975,  2.5671,\n",
       "        2.7344,  2.8016,  2.9634,  3.0665,  2.8715,  2.5403,  2.9084,\n",
       "        2.8237,  3.0334,  3.0993,  2.8965,  2.5329,  2.4298,  2.206 ,\n",
       "        2.3053,  2.1007,  2.1705,  1.9512,  3.4242,  3.3317,  3.4106,\n",
       "        4.0983,  3.724 ,  3.9917,  3.5459,  3.1523,  1.8209,  1.7554,\n",
       "        1.8226,  1.8681,  2.1099,  2.3817,  2.3817,  2.7986,  2.7986,\n",
       "        2.8927,  2.4091,  2.3999,  2.4982,  2.4786,  2.2875,  2.1675,\n",
       "        2.3889,  2.505 ])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array(df['DIS'])\n",
    "z\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_boston_data_new():\n",
    "    data = load_boston()\n",
    "    X, y = data['data'], data['target']\n",
    "    \n",
    "    X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3])#корень из пятой фичи и куб из шестой\n",
    "    \n",
    "    #3.6.2\n",
    "    #Добавьте к признакам нелинейной модели квадрат признака DIS (и переобучите модель. Какой получился RMSE?)\n",
    "    #X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3, X[:, 7:8] ** 2])\n",
    "    \n",
    "    # Нормализовать даннные с помощью стандартной нормализации\n",
    "    #X = (X - X.mean(axis=0)) / X.std(axis=0) #comment for task 3.6.2\n",
    "    # Добавить фиктивный столбец единиц (bias линейной модели)\n",
    "    X = np.hstack([np.ones(X.shape[0])[:, np.newaxis], X])\n",
    "    \n",
    "    \n",
    "#     df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "#     z = df['DIS']**2\n",
    "#     X = np.hstack([np.ones(z), X])\n",
    "#     print(z)\n",
    "    \n",
    "#     X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3])\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы добавили два новых признака:\n",
    "1. Взяли корень из признака RM (среднее число комнат на сожителя)\n",
    "1. Возвели в куб значения признака AGE\n",
    "\n",
    "Это только два примера. Всевозможных комбинаций признаков и примененных к ним функций неограниченное количество."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_validate(X, y):\n",
    "    # Разбить данные на train/valid\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=1)\n",
    "\n",
    "    # Создать и обучить линейную регрессию\n",
    "    linreg_alg = LinRegAlgebra()\n",
    "    linreg_alg.fit(X_train, y_train)\n",
    "\n",
    "    # Сделать предсказания по валидционной выборке\n",
    "    y_pred = linreg_alg.predict(X_valid)\n",
    "\n",
    "    # Посчитать значение ошибок MSE и RMSE для валидационных данных\n",
    "    print_regression_metrics(y_valid, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 23.38, RMSE = 4.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dex/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Подготовить данные без модификации признаков\n",
    "X, y = prepare_boston_data()\n",
    "# Провести эксперимент\n",
    "train_validate(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dex/.local/lib/python3.8/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function load_boston is deprecated; `load_boston` is deprecated in 1.0 and will be removed in 1.2.\n",
      "\n",
      "    The Boston housing prices dataset has an ethical problem. You can refer to\n",
      "    the documentation of this function for further details.\n",
      "\n",
      "    The scikit-learn maintainers therefore strongly discourage the use of this\n",
      "    dataset unless the purpose of the code is to study and educate about\n",
      "    ethical issues in data science and machine learning.\n",
      "\n",
      "    In this special case, you can fetch the dataset from the original\n",
      "    source::\n",
      "\n",
      "        import pandas as pd\n",
      "        import numpy as np\n",
      "\n",
      "\n",
      "        data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
      "        raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
      "        data = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
      "        target = raw_df.values[1::2, 2]\n",
      "\n",
      "    Alternative datasets include the California housing dataset (i.e.\n",
      "    :func:`~sklearn.datasets.fetch_california_housing`) and the Ames housing\n",
      "    dataset. You can load the datasets as follows::\n",
      "\n",
      "        from sklearn.datasets import fetch_california_housing\n",
      "        housing = fetch_california_housing()\n",
      "\n",
      "    for the California housing dataset and::\n",
      "\n",
      "        from sklearn.datasets import fetch_openml\n",
      "        housing = fetch_openml(name=\"house_prices\", as_frame=True)\n",
      "\n",
      "    for the Ames housing dataset.\n",
      "    \n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 14.28, RMSE = 3.78\n"
     ]
    }
   ],
   "source": [
    "# Подготовить данные без модификации признаков\n",
    "X, y = prepare_boston_data_new()\n",
    "# Провести эксперимент\n",
    "train_validate(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Задание 3.6.2\n",
    "Добавьте к признакам нелинейной модели квадрат признака `DIS` и переобучите модель. Какой получился `RMSE`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RMSE = 3.69 from code above is correct answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.6.3\n",
    "\n",
    "Уберите нормализацию и оставьте добавленные признаки на основе RM и AGE. Какой получился RMSE? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with\n",
    "- X = np.hstack([X, np.sqrt(X[:, 5:6]), X[:, 6:7] ** 3])#корень из пятой фичи и куб из шестой\n",
    "- and  #X = (X - X.mean(axis=0)) / X.std(axis=0) #comment for task 3.6.2\n",
    "\n",
    "RMSE = 3.78 is correct answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
